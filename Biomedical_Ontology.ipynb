{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\satya\\appdata\\local\\pip\\cache\\wheels\\b1\\1a\\8f\\a4c34be976825a2f7948d0fa40907598d69834f8ab5889de11\\pypdf2-1.26.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in c:\\users\\satya\\anaconda3\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\satya\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\satya\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\satya\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\satya\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\satya\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2  numpy nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "# Playing with Original Dictionary\n",
    "lst = list((words.words()))\n",
    "l = np.asarray(lst)\n",
    "st = set()\n",
    "for each in l:\n",
    "    st.add(each.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether extracted word exist in real world dictionary or not\n",
    "def binary_search(word):\n",
    "    if word in st:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z]')\n",
    "file = open(\"book.pdf\", 'rb')\n",
    "pdfFileReader = PyPDF2.PdfFileReader(file)\n",
    "totalpages = pdfFileReader.numPages\n",
    "currentpage = 0\n",
    "text = ''\n",
    "while (currentpage < totalpages ):\n",
    "    data = pdfFileReader.getPage(currentpage)\n",
    "    temptext=data.extractText()\n",
    "    text = text + \" \" + regex.sub(' ',temptext.lower())\n",
    "    currentpage += 1\n",
    "ans  = \"\"\n",
    "for word in text.split():\n",
    "    if binary_search(word):\n",
    "        ans = ans + ' ' + word\n",
    "        \n",
    "with open(\"biodata.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ride of stopwords\n",
    "with open(\"biodata.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "text = ''\n",
    "for each in data.split():\n",
    "    if len(each) > 3 and each not in stopwords.words('English'):\n",
    "        text = text + ' ' + each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram generation and counting their frequencies\n",
    "\n",
    "token = word_tokenize(text)\n",
    "wordcount  = {}\n",
    "for word in token:\n",
    "    if word not in wordcount.keys():\n",
    "        wordcount[word]=1\n",
    "    else :\n",
    "        wordcount[word]+=1\n",
    "with open('Unigram.txt','w',encoding='utf-8') as f:\n",
    "    f.write(json.dumps(wordcount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram generation and counting their frequenccies\n",
    "\n",
    "bigram =  list(ngrams(token,2))\n",
    "bgram = {}\n",
    "for each in bigram:\n",
    "    if each[0] == each[1]:\n",
    "        continue\n",
    "    word = each[0] + ' ' + each[1]\n",
    "    if word not in bgram.keys():\n",
    "        bgram[word] = 1\n",
    "    else :\n",
    "        bgram[word]+=1\n",
    "with open('Bigram.txt','w',encoding='utf-8') as file:\n",
    "    file.write(json.dumps(bgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class owl:\n",
    "    def __init__(self,n,f):\n",
    "        self.oname=n\n",
    "        self.header='''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n <Ontology xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"    xsi:schemaLocation=\"http://www.w3.org/2002/07/owl# http://www.w3.org/2009/09/owl2-xml.xsd\" xmlns=\"http://www.w3.org/2002/07/owl#\" xml:base=\"http://example.com/'''+self.oname+'''\" ontologyIRI=\"http://example.com/'''+self.oname+'''\" > <Prefix name=\"CF\" IRI=\"'''+self.oname+'''#\"/> <Import>http://example.com/'''+self.oname+'''</Import>\\n\\n'''\n",
    "        self.content=[]\n",
    "        self.footer='\\n</Ontology>'\n",
    "        self.file=f\n",
    "\n",
    "    def cont(self):\n",
    "        t=''\n",
    "        for i in self.content:\n",
    "            t+=i+'\\n'\n",
    "        return t    \n",
    "    def subclass(self,sub,sup):\n",
    "        c='<SubClassOf><Class IRI=\"#'+sub+'\"/> <Class IRI=\"#'+sup+'\"/></SubClassOf>\\n'\n",
    "        self.content.append(c)\n",
    "\n",
    "    def properti(self,sub,prop,sup):\n",
    "        c='<SubClassOf>   <Class IRI=\"#'+sub+'\"/> <ObjectAllValuesFrom>  <ObjectProperty IRI=\"#'+prop+'\"/><Class IRI=\"#'+sup+'\"/> </ObjectAllValuesFrom>    </SubClassOf>'\n",
    "        self.content.append(c)\n",
    "\n",
    "    def write(self):\n",
    "        f=open(self.file,'w')\n",
    "        f.write(self.header)\n",
    "        f.write(self.cont())\n",
    "        f.write(self.footer)\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Owl File\n",
    "lst = set()\n",
    "with open('wordlist.txt','r',encoding = 'utf-8') as file:\n",
    "    wordlist = file.read()\n",
    "l = ['epidemiology','biological','epidemiological','disease','medical','transformation','instrument','anthropology','cardiovascular','psychologically','odontology','electron','microscopy','bromide']\n",
    "for each in list(bigram):\n",
    "    if each[0] == each[1]:\n",
    "        continue\n",
    "    else:\n",
    "        if (each[0] in l or each[1] in l) and (each[0] not in wordlist.split() and each[1] not in wordlist.split()):\n",
    "            trex=nltk.pos_tag(each)\n",
    "            if trex[0][1].startswith('N') and trex[1][1].startswith('N'):  \n",
    "                # or (trex[0][1]=='JJ' and trex[1][1]=='JJ') or ((trex[0][1].startswith('N') and trex[1][1]=='JJ')) or ((trex[0][1]=='JJ' and trex[1][1].startswith('N')))):\n",
    "                lst.add(each)\n",
    "lst = list(lst)\n",
    "o=owl('BioMedical','test.owl')\n",
    "for tup in lst:\n",
    "    if tup[0] in l:\n",
    "        o.subclass(tup[1],tup[0])\n",
    "    else:\n",
    "        o.subclass(tup[0],tup[1])\n",
    "o.write()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Method\n",
    "\n",
    "fo = open(\"biodata.txt\",encoding=\"utf-8\")\n",
    "fo1 = fo.readlines()\n",
    "y=0\n",
    "mk=\"\"\n",
    "for line in fo1:\n",
    "       bigm = list(nltk.bigrams(line.split()))\n",
    "       bigmC = Counter(bigm)\n",
    "       for key, value in sorted(bigmC.items(), key=lambda t:t[-1], reverse=True):\n",
    "           mk=mk+ str(key)+ ' ' + str(value) + '\\n'\n",
    "\n",
    "mname=\"newbram.txt\"     \n",
    "with open(mname, \"w+\", encoding=\"utf-8\") as f:\n",
    "    f.write(mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d1 = re.findall(r\"(\\w+) in the\", data)\n",
    "d2 = re.findall(r\"in the (\\w+)\", data)\n",
    "d3 = re.findall(r\"(\\w+) is a\", data)\n",
    "d4 = re.findall(r\"is a (\\w+)\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 211\n"
     ]
    }
   ],
   "source": [
    "print(len(d3),len(d4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "newlst = set()\n",
    "while e < len(d1):\n",
    "    each = (d1[e],d2[e])\n",
    "    if d1[e] == d2[e]:\n",
    "        e  = e + 1\n",
    "        continue\n",
    "    else:\n",
    "        if len(each[0])>3  and len(each[1]):\n",
    "            trex=nltk.pos_tag(each)\n",
    "            if trex[0][1].startswith('N') and trex[1][1].startswith('N'):\n",
    "                newlst.add(each)\n",
    "    e  = e + 1\n",
    "e = 0\n",
    "while e < len(d4):\n",
    "    each = (d3[e],d4[e])\n",
    "    if d3[e] == d4[e]:\n",
    "        e  = e + 1\n",
    "        continue\n",
    "    else:\n",
    "        if len(each[0])>3  and len(each[1]):\n",
    "            trex=nltk.pos_tag(each)\n",
    "            if trex[0][1].startswith('N') and trex[1][1].startswith('N'):\n",
    "                newlst.add(each)\n",
    "    e  = e + 1\n",
    "lst = list(newlst)\n",
    "o=owl('BioMedical','trex.owl')\n",
    "for tup in lst:\n",
    "    if tup[0] in l:\n",
    "        o.subclass(tup[1],tup[0])\n",
    "    else:\n",
    "        o.subclass(tup[0],tup[1])\n",
    "o.write()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
